
Guided Exercise: Profiling Degraded Worker Nodes
In this exercise, you will profile a degraded worker node and utilize proper corrective measures to restore node functionality.

Outcomes

You should be able to:

Identify cluster issues resulting from an unhealthy worker node.

Inspect and isolate the specific cause of a worker node degradation.

Repair the issue and restore proper worker node functionality.

To perform this exercise, ensure you have access to:

A running OpenShift cluster.

Access to the workstation virtual machine and cluster administrator credentials.

As the student user on the workstation machine, use the lab command to prepare your system for this exercise.

This command ensures that the cluster API is reachable.

[student@workstation ~]$ lab workers-degrade start
Procedure 11.1. Instructions

Log in to your OpenShift cluster as the admin user.

[student@workstation ~]$ oc login -u admin -p redhat \
> https://api.ocp4.example.com:6443
Login successful.
...output omitted...
From the workstation machine, inspect the OpenShift web console Monitoring page for alerts that provide information regarding cluster performance degradation.

Display the address for the web console by running the command:

[student@workstation ~]$ oc whoami --show-console
https://console-openshift-console.apps.ocp4.example.com
Open the link provided in the command output in a browser, and then log in as the admin user with the redhat password.

Navigate to the Monitoring section of the web console and inspect the Firing tab found on the Alerting page. After a few minutes, several alerts with a severity of Warning are displayed with additional AlertManager notifications. The alerts provide details about the cluster degradation, cluster member absence, and make explicit mention of worker01 being unready.

Utilize CLI commands to profile the status of the worker nodes to determine the appropriate remediation.

Check the status of all nodes. Notice that one worker node reports a NotReady status.

[student@workstation ~]$ oc get nodes
NAME       STATUS     ROLES    AGE   VERSION
master01   Ready      master   ...   ...
master02   Ready      master   ...   ...
master03   Ready      master   ...   ...
worker01   NotReady   worker   ...   ...
worker02   Ready      worker   ...   ...
worker03   Ready      worker   ...   ...
worker04   Ready      infra    ...   ...
worker05   Ready      infra    ...   ...
worker06   Ready      infra    ...   ...
Inspect the worker01 node showing a Not Ready status using the oc describe command.

[student@workstation ~]$ oc describe node/worker01
...output omitted...
Taints:             node.kubernetes.io/not-ready:NoExecute
                    node.kubernetes.io/not-ready:NoSchedule
...output omitted...
Conditions:
Type        Status  ...     Reason                 Message
...output omitted...
Ready       False   ...     KubeletNotReady        [container runtime is down...
...output omitted...
OpenShift is unable to schedule pods for this worker node because the worker01 node has NoSchedule and NoExecute taints. The Conditions section shows that the container run time (cri-o) is down, which is the reason for the taints and Not Ready status.

Use the oc debug command to log in to the cluster and attempt remediation. The oc debug command fails to start the debug pod because the worker node is in a Not Ready state. Press Ctrl+C to stop the oc debug command.

[student@workstation ~]$ oc debug node/worker01
Starting pod/worker01-debug ...
To use host binaries, run `chroot /host`
^C
Removing debug pod ...
Inspect the OpenShift services and correct the outage for the crio service on the worker node.

Check the status of crio service for worker01 to reveal that the service is not running.

[student@workstation ~]$ ssh core@worker01 "sudo systemctl is-active crio"
inactive
Start crio service for worker01.

[student@workstation ~]$ ssh core@worker01 "sudo systemctl start crio"
Check status of crio service for worker01 to confirm that the service is running.

[student@workstation ~]$ ssh core@worker01 "sudo systemctl is-active crio"
active
Verify the functionality of deployments on the worker node, and also that all monitoring alerts are resolved.

Check the status of all Nodes and verify the node has been properly restored. It may take several minutes for the worker01 node to return to the Ready status.

[student@workstation ~]$ oc get nodes
NAME       STATUS   ROLES    AGE   VERSION
master01   Ready    master   ...   ...
master02   Ready    master   ...   ...
master03   Ready    master   ...   ...
worker01   Ready    worker   ...   ...
worker02   Ready    worker   ...   ...
worker03   Ready    worker   ...   ...
worker04   Ready    infra    ...   ...
worker05   Ready    infra    ...   ...
worker06   Ready    infra    ...   ...
Inspect the worker01 node for taints using the oc describe command.

[student@workstation ~]$ oc describe node/worker01 | grep -i taints
Taints:         <none>
Worker node worker01 is healthy and no longer displays any taints.

Finish

On the workstation machine, use the lab command to complete this exercise. This is important to ensure that resources from previous exercises do not impact upcoming exercises.

[student@workstation ~]$ lab workers-degrade finish
This concludes the section.