Guided Exercise: Controlling Pod Scheduling Behavior
In this exercise, you will configure an application to run on a subset of the cluster worker nodes.

Outcomes

You should be able to use the OpenShift command-line interface to:

Add a new label to a node.

Deploy pods to nodes that match a specified label.

Remove a label from a node.

Troubleshoot when pods fail to deploy to a node.

As the student user on the workstation machine, use the lab command to prepare your system for this exercise.

The command ensures that the cluster API is reachable and creates a project that you will be using in the activity.

[student@workstation ~]$ lab schedule-pods start
As the developer user, create a new project named schedule-pods.

Log in to your OpenShift cluster as the developer user.

[student@workstation ~]$ oc login -u developer -p developer \
>    https://api.ocp4.example.com:6443
Login successful.
...output omitted...
Create a new project named schedule-pods.

[student@workstation ~]$ oc new-project schedule-pods
Now using project "schedule-pods" on server "https://api.ocp4.example.com".
...output omitted...
Deploy and scale a test application.

Create a new application named hello using the container located at quay.io/redhattraining/hello-world-nginx:v1.0.

[student@workstation ~]$ oc new-app --name hello \
>    --docker-image quay.io/redhattraining/hello-world-nginx:v1.0
...output omitted...
--> Creating resources ...
    imagestream.image.openshift.io "hello" created
    deployment.apps "hello" created
    service "hello" created
--> Success
...output omitted...
Create a route to the application.

[student@workstation ~]$ oc expose svc/hello
route.route.openshift.io/hello exposed
Manually scale the application so there are four running pods.

[student@workstation ~]$ oc scale --replicas 4 deployment/hello
deployment.apps/hello scaled
Verify that the four running pods are distributed between the nodes.

[student@workstation ~]$ oc get pods -o wide
NAME                     READY   STATUS    ...   IP           NODE       ...
hello-6c4984d949-78qsp   1/1     Running   ...   10.9.0.30    master02   ...
hello-6c4984d949-cf6tb   1/1     Running   ...   10.10.0.20   master01   ...
hello-6c4984d949-kwgbg   1/1     Running   ...   10.8.0.38    master03   ...
hello-6c4984d949-mb8z7   1/1     Running   ...   10.10.0.19   master01   ...
NOTE
Depending on the existing load on each node, your output may be different. Although the scheduler will attempt to distribute the pods, the distribution may not be even.

Prepare the nodes so that application workloads can be distributed to either development or production nodes by assigning the env label. Assign the env=dev label to the master01 node and the env=prod label to the master02 node.

Log in to your OpenShift cluster as the admin user. A regular user does not have permission to view information about nodes and cannot label nodes.

[student@workstation ~]$ oc login -u admin -p redhat
Login successful.
...output omitted...
Verify that none of the nodes use the env label.

[student@workstation ~]$ oc get nodes -L env
NAME       STATUS   ROLES           AGE     VERSION           ENV
master01   Ready    master,worker   5d18h   v1.18.3+012b3ec
master02   Ready    master,worker   5d18h   v1.18.3+012b3ec
master03   Ready    master,worker   5d18h   v1.18.3+012b3ec
Add the env=dev label to the master01 node to indicate that it is a development node.

[student@workstation ~]$ oc label node master01 env=dev
node/master01 labeled
Add the env=prod label to the master02 node to indicate that it is a production node.

[student@workstation ~]$ oc label node master02 env=prod
node/master02 labeled
Verify that the nodes have the correct env label set. Make note of the node that has the env=dev label, as you will check later to see if the application pods have been deployed to that node.

[student@workstation ~]$ oc get nodes -L env
NAME       STATUS   ROLES           AGE     VERSION           ENV
master01   Ready    master,worker   5d18h   v1.18.3+012b3ec   dev
master02   Ready    master,worker   5d18h   v1.18.3+012b3ec   prod
master03   Ready    master,worker   5d18h   v1.18.3+012b3ec
Switch back to the developer user and modify the hello application so that it is deployed to the development node. After verifying this change, delete the schedule-pods project.

Log in to your OpenShift cluster as the developer user.

[student@workstation ~]$ oc login -u developer -p developer
Login successful.
...output omitted...
Using project "schedule-pods".
Modify the deployment resource for the hello application to select a development node. Make sure to add the node selector the spec group in the template section.

[student@workstation ~]$ oc edit deployment/hello
Add the highlighted lines below to the deployment resource, indenting as shown.

...output omitted...
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      nodeSelector:
        env: dev
      restartPolicy: Always
...output omitted...
The following output from oc edit is displayed after you save your changes.

deployment.apps/hello edited
Verify that the application pods are deployed to the node with the env=dev label. Although it make take a little time to redeploy, the application pods must be deployed to the master01 node.

[student@workstation ~]$ oc get pods -o wide
NAME                   READY  STATUS   RESTARTS  AGE  IP          NODE      ...
hello-b556ccf8b-8scxd  1/1    Running  0         80s  10.10.0.14  master01  ...
hello-b556ccf8b-hb24w  1/1    Running  0         77s  10.10.0.16  master01  ...
hello-b556ccf8b-qxlj8  1/1    Running  0         80s  10.10.0.15  master01  ...
hello-b556ccf8b-sdxpj  1/1    Running  0         76s  10.10.0.17  master01  ...
Remove the schedule-pods project.

[student@workstation ~]$ oc delete project schedule-pods
project.project.openshift.io "schedule-pods" deleted
Finish cleaning up this portion of the exercise by removing the env label from all nodes.

Log in to your OpenShift cluster as the admin user.

[student@workstation ~]$ oc login -u admin -p redhat
Login successful.
...output omitted...
Remove the env label from all nodes that have it.

[student@workstation ~]$ oc label node -l env env-
node/master01 labeled
node/master02 labeled
The schedule-pods-ts project contains an application that runs only on nodes that are labeled as client=ACME. In the following example, the application pod is pending and you must diagnose the problem using the following steps:

Log in to your OpenShift cluster as the developer user and ensure that you are using the schedule-pods-ts project.

[student@workstation ~]$ oc login -u developer -p developer
Login successful.
...output omitted...
Using project "schedule-pods-ts".
If the output above does not show that you are using the schedule-pods-ts project, switch to it.

[student@workstation ~]$ oc project schedule-pods-ts
Now using project "schedule-pods-ts" on server
"https://api.ocp4.example.com:6443".
Verify that the application pod has a status of Pending.

[student@workstation ~]$ oc get pods
NAME                       READY   STATUS    RESTARTS   AGE
hello-ts-5dbff9f44-w6csj   0/1     Pending   0          6m19s
Because a pod with a status of pending does not have any logs, check details of the pod using the oc describe pod command to see if describing the pod provides any useful information.

[student@workstation ~]$ oc describe pod hello-ts-5dbff9f44-8h7c7
...output omitted...
QoS Class:       BestEffort
Node-Selectors:  client=acme
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            ...               Message
  ----     ------            ...               -------
  Warning  FailedScheduling  ...  0/3 nodes are available: 3 node(s) didn't match node selector.
Based on this information, the pod should be scheduled to a node with the label client=acme, but none of the three nodes have this label. The information provided indicates that at least one worker node has the label client=ACME. You have found the problem. The application must be modified so that it uses the correct node selector.

Edit the deployment resource for the application to use the correct node selector.

[student@workstation ~]$ oc edit deployment/hello-ts
Change acme to ACME as shown below.

...output omitted...
      dnsPolicy: ClusterFirst
      nodeSelector:
        client: ACME
      restartPolicy: Always
...output omitted...
The following output from oc edit is displayed after you save your changes.

deployment.apps/hello-ts edited
Verify that the a new application pod is deployed and has a status of Running.

[student@workstation ~]$ oc get pods
NAME                        READY   STATUS    RESTARTS   AGE
hello-ts-69769f64b4-wwhpc   1/1     Running   0          11s
Finish

On the workstation machine, use the lab command to complete this exercise. This is important to ensure that resources from previous exercises do not impact upcoming exercises.

[student@workstation ~]$ lab schedule-pods finish
This concludes the lab.