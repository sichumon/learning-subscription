Lab: Managing Storage
Performance Checklist

In this lab, you will manage persistent volumes for OpenStack compute instances. You will also manage shared file systems and use them from OpenStack compute instances.

Outcomes

You should be able to:

Administer OpenStack block storage.

Administer OpenStack shared file systems.

Log in to workstation as student using student as the password.

On workstation, run lab storage-review setup, which verifies OpenStack services and previously created resources. This script also ensures that the required resources to proceed with the exercise are in place.

[student@workstation ~]$ lab storage-review setup
As operator1, create a 10 GB bootable volume called production-volume1 in the production project using the rhel7 image.

Source the operator1 user's credentials.

[student@workstation ~]$ source operator1-production-rc
[student@workstation ~(operator1-production)]$ 
Create a 10 GB bootable volume called production-volume1 using the rhel7 image.

[student@workstation ~(operator1-production)]$ openstack volume create \
--image rhel7 \
--size 10 \
production-volume1
...output omitted...
NOTE
After executing the openstack volume create command, wait for up to two minutes while the contents from the rhel7 image is extracted to the production-vol1 volume. You may also use the watch openstack volume list command to monitor the real-time status of the production-vol1 volume.

Verify that the production-volume1 volume is available.

[student@workstation ~(operator1-production)]$ openstack volume list \
-c Name \
-c Status
+--------------------+-----------+
| Name               | Status    |
+--------------------+-----------+
| production-volume1 | available |
+--------------------+-----------+
As operator1, create an empty 1 GB volume called production-volume2 in the production project.

Create an empty 1 GB volume called production-volume2.

[student@workstation ~(operator1-production)]$ openstack volume create \
--size 1 \
production-volume2
...output omitted...
Verify that the production-volume2 volume is available.

[student@workstation ~(operator1-production)]$ openstack volume list \
-c Name \
-c Status
+--------------------+-----------+
| Name               | Status    |
+--------------------+-----------+
| production-volume2 | available |
| production-volume1 | available |
+--------------------+-----------+
Verify that the new volumes exist as objects in the volumes Ceph pool.

NOTE
The OpenStack installation in the classroom environment uses Red Hat Ceph Storage as the back-end storage provider for the OpenStack Block Storage service.

Find the unique IDs of production-volume1 and production-volume2. Use these IDs to compare with the objects in the volumes Ceph pool.

[student@workstation ~(operator1-production)]$ openstack volume list \
-c Name \
-c ID
+--------------------------------------+--------------------+
| ID                                   | Name               |
+--------------------------------------+--------------------+
| 0b4944e7-6939-4350-86a4-71746c6fe4f3 | production-volume2 |
| 03c4ebcd-1829-4f08-8bdd-9d8e8dcf14ff | production-volume1 |
+--------------------------------------+--------------------+
Verify that the production-volume1 volume exists as an object in the volumes Ceph pool. Use the same ID of production-volume1 as in the preceding output.

[student@workstation ~(operator1-production)]$ ssh root@controller0 \
rados -p volumes \
ls | grep 03c4ebcd-1829-4f08-8bdd-9d8e8dcf14ff
rbd_id.volume-03c4ebcd-1829-4f08-8bdd-9d8e8dcf14ff
Verify that the production-volume2 volume exists as an object in the volumes Ceph pool. Use the same ID of production-volume2 as in the preceding output.

[student@workstation ~(operator1-production)]$ ssh root@controller0 \
rados -p volumes \
ls | grep 0b4944e7-6939-4350-86a4-71746c6fe4f3
rbd_id.volume-0b4944e7-6939-4350-86a4-71746c6fe4f3
As operator1, launch an instance with the following parameters in the production project using the production-volume1 bootable volume.

name: production-server1

flavor: default

key-name: operator1-keypair

nic: net-id=production-network1

volume: production-volume1

Attach the empty production-volume2 volume to the production-server1 instance.

Launch the production-server1 instance using the aforementioned parameters.

[student@workstation ~(operator1-production)]$ openstack server create \
--flavor default \
--volume production-volume1 \
--key-name operator1-keypair \
--nic net-id=production-network1 \
production-server1 --wait
...output omitted...
Attach the empty production-volume2 volume to the production-server1 instance.

[student@workstation ~(operator1-production)]$ openstack server add \
volume production-server1 production-volume2
Access the production-server1 instance and verify that the empty production-volume2 volume was successfully attached to the production-server1 instance.

Associate 172.25.250.151 as the floating IP address to the production-server1 instance.

[student@workstation ~(operator1-production)]$ openstack server add \
floating ip production-server1 172.25.250.151
The floating IP address 172.25.250.151 became available as a part of the lab setup.

Verify that an additional 1 GB disk representing production-volume2 appears among the existing disks in the production-server1 instance.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151 lsblk
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
vda    253:0    0  10G  0 disk
└─vda1 253:1    0  10G  0 part /
vdb    253:16   0   1G  0 disk 
Create a 1 GB partition in production-volume2 that appears as the device vdb in the partition table of the production-server1 instance. Format the partition with the xfs file system type and mount it temporarily on /mnt. Create the file /mnt/hello.txt with hello as the text message in it. The file /mnt/hello.txt gets accommodated into the production-volume2 as /mnt acts as the mount point for the volume. Also, create the file /hello2.txt with hello2 as the text message in it. The file /hello2.txt gets accommodated into the production-volume1 because you are using the volume as the root file system in the production-server1 instance.

Create a primary partition of 1 GB in the production-volume2 that appears as the device vdb in the partition table of the production-server1 instance.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151 \
sudo /usr/sbin/parted /dev/vdb \
mklabel msdos \
mkpart primary 0 1G
Warning: The resulting partition is not properly aligned for best performance.
Information: You may need to update /etc/fstab.
            
NOTE
You may ignore the warning message in the output of the preceding command.

Create a file system of type xfs in the /dev/vdb1 partition.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151 \
sudo /usr/sbin/mkfs \
-t xfs /dev/vdb1
meta-data=/dev/vdb1              isize=512    agcount=4, agsize=65536 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=262143, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=855, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
Mount /dev/vdb1 on /mnt temporarily.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151 \
sudo /usr/bin/mount \
/dev/vdb1 /mnt
Create the text files /mnt/hello.txt and /hello2.txt with the text messages hello and hello2 respectively.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151
[cloud-user@production-server1 ~]$ sudo -i
[root@production-server1 ~]# echo hello > /mnt/hello.txt
[root@production-server1 ~]# echo hello2 > /hello2.txt
[root@production-server1 ~]# sync
IMPORTANT
The sync command flushes contents from the Linux buffer cache to the disk. Skipping this command may cost you the contents of the text files you created.

Log off from production-server1.

[root@production-server1 ~]# logout
[cloud-user@production-server1 ~]$ logout
Connection to 172.25.250.151 closed.
[student@workstation ~(operator1-production)]$ 
Delete the existing production-server1 instance and create a fresh instance with production-volume1 as the bootable volume using the following parameters.

name: production-server1

flavor: default

key-name: operator1-keypair

nic: net-id=production-network1

volume: production-volume1

Attach the production-volume2 volume to the new production-server1 instance.

Delete the existing production-server1 instance.

[student@workstation ~(operator1-production)]$ openstack server delete \
production-server1
Create a fresh instance using the aforementioned parameters.

[student@workstation ~(operator1-production)]$ openstack server create \
--flavor default \
--volume production-volume1 \
--key-name operator1-keypair \
--nic net-id=production-network1 \
production-server1 --wait
...output omitted...
Attach production-volume2 to production-server1.

[student@workstation ~(operator1-production)]$ openstack server add \
volume production-server1 production-volume2
Confirm that the text files you created earlier persist, with all of the content in the production-server1 instance.

Associate 172.25.250.151 as the floating IP address to the production-server1 instance.

[student@workstation ~(operator1-production)]$ openstack server add \
floating ip production-server1 172.25.250.151
Mount /dev/vdb1 on /mnt temporarily.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151 \
sudo /usr/bin/mount /dev/vdb1 /mnt
Verify the contents of the /mnt/hello.txt and /hello2.txt files.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151 \
cat /mnt/hello.txt
hello
[student@workstation ~(operator1-production)]$ ssh 172.25.250.151 \
cat /hello2.txt
hello2
As operator1, create a 1 GB shared file system in the production project. Set the name of the shared file system to production-share1, and the type to cephfstype. Use the cephfs protocol while creating production-share1. For administrative operations such as managing the types of the shared file systems, use the architect1 tenant user.

NOTE
The preinstalled Red Hat Ceph Storage in the classroom environment backs the production-share1 shared file system. The OpenStack Shared File Systems service uses the appropriate Ceph driver to coordinate with Red Hat Ceph Storage.

Source the architect1 user's credentials.

[student@workstation ~(operator1-production)]$ source architect1-production-rc
[student@workstation ~(architect1-production)]$ 
Verify the status of the OpenStack Shared File Systems services.

[student@workstation ~(architect1-production)]$ manila service-list \
--columns Binary,Status,State
+------------------+---------+-------+
| Binary           | Status  | State |
+------------------+---------+-------+
| manila-scheduler | enabled | up    |
| manila-share     | enabled | up    |
+------------------+---------+-------+
Create the share type cephfstype. Set the value of driver_handles_share_servers to false.

[student@workstation ~(architect1-production)]$ manila type-create \
cephfstype false
+----------------------+--------------------------------------+
| Property             | Value                                |
+----------------------+--------------------------------------+
| required_extra_specs | driver_handles_share_servers : False |
| Name                 | cephfstype                           |
| Visibility           | public                               |
| is_default           | -                                    |
| ID                   | 384aca36-841a-4344-a75b-421e1d096e47 |
| optional_extra_specs |                                      |
| Description          | None                                 |
+----------------------+--------------------------------------+
IMPORTANT
To dynamically scale the share nodes, set the value of driver_handles_share_servers to true.

As operator1, create a 1 GB shared file system called production-share1.

[student@workstation ~(architect1-production)]$ source operator1-production-rc
[student@workstation ~(operator1-production)]$ manila create \
--name production-share1 --share-type cephfstype cephfs 1
...output omitted...
Verify that production-share1 was successfully created.

[student@workstation ~(operator1-production)]$ manila list \
--columns Name,Size,'Share Proto',Status,'Share Type Name'
+-------------------+------+-------------+-----------+-----------------+
| Name              | Size | Share Proto | Status    | Share Type Name |
+-------------------+------+-------------+-----------+-----------------+
| production-share1 | 1    | CEPHFS      | available | cephfstype      |
+-------------------+------+-------------+-----------+-----------------+
Attach an additional NIC from the provider-storage network to the production-server1 instance in order to reach the network segment of the production-share1 shared file system from the production-server1 instance. Use DHCP as the configuration method for the additional NIC.

Attach an additional NIC from production-network1 to production-server1.

[student@workstation ~(operator1-production)]$ openstack server add \
network production-server1 provider-storage
Open an SSH session to production-server1 and confirm the existence of the additional NIC in the instance.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151
[cloud-user@production-server1 ~]$ ip a s
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1442 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:b1:87:d9 brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.11/24 brd 192.168.1.255 scope global noprefixroute dynamic eth0
       valid_lft 42295sec preferred_lft 42295sec
    inet6 fe80::f816:3eff:feb1:87d9/64 scope link
       valid_lft forever preferred_lft forever
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:2e:e2:2d brd ff:ff:ff:ff:ff:ff
Notice the eth1 network interface that you added in the preceding step.

Create the /etc/sysconfig/network-scripts/ifcfg-eth1 file and set DHCP as the configuration method for eth1. Use the sudo command when creating the interface file because this operation requires root privileges.

DEVICE=eth1
ONBOOT=yes
BOOTPROTO=dhcp
Bring up the eth1 interface.

[cloud-user@production-server1 ~]$ sudo ifup eth1
Verify the settings of eth1.

[cloud-user@production-server1 ~]$ ip a s
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1442 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:b1:87:d9 brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.11/24 brd 192.168.1.255 scope global noprefixroute dynamic eth0
       valid_lft 41615sec preferred_lft 41615sec
    inet6 fe80::f816:3eff:feb1:87d9/64 scope link
       valid_lft forever preferred_lft forever
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:2e:e2:2d brd ff:ff:ff:ff:ff:ff
    inet 172.24.3.205/24 brd 172.24.3.255 scope global noprefixroute dynamic eth1
       valid_lft 43100sec preferred_lft 43100sec
    inet6 fe80::f816:3eff:fe2e:e22d/64 scope link
       valid_lft forever preferred_lft forever
Notice the 172.24.3.205 IP address of the eth1 interface. This IP address allows you to reach the network of production-share1 shared file system from the production-server1 instance.

Log off from production-server1.

[cloud-user@production-server1 ~]$ logout
Connection to 172.25.250.151 closed.
[student@workstation ~(operator1-production)]$ 
Create the cephx user client.cloud-user and save the key ring as cloud-user.keyring. Authenticate as client.manila using the key ring from /etc/ceph/ceph.client.manila.keyring while creating client.cloud-user. Transfer the appropriate Ceph configuration file and the cloud-user.keyring file to production-server1. Install any required utilities on production-server1 to access the Ceph-backed shared file system using the repository configuration file from http://materials.example.com/ceph.repo.

Open an SSH session to controller0 that acts as the Ceph Monitor and has all the required administrative tools.

[student@workstation ~(operator1-production)]$ ssh root@controller0
[root@controller0 ~]# 
Create the cephx user client.cloud-user and save its key ring to /root/cloud-user.keyring. Authenticate as continual using the key ring from /etc/ceph/ceph.client.manila.keyring while creating canticles.

[root@controller0 ~]# ceph --name=client.manila \
--keyring=/etc/ceph/ceph.client.manila.keyring \
auth get-or-create client.cloud-user > /root/cloud-user.keyring
Log off from controller0.

[root@controller0 ~]# logout
Connection to controller0 closed.
[student@workstation ~(operator1-production)]$ 
Transfer the appropriate Ceph configuration file and the cloud-user.keyring file to production-server1.

[student@workstation ~(operator1-production)]$ scp \
root@controller0:{/etc/ceph/ceph.conf,/root/cloud-user.keyring} manila/
ceph.conf                                     100%  797   870.4KB/s   00:00
cloud-user.keyring                            100%   68    75.9KB/s   00:00
[student@workstation ~(operator1-production)]$ scp \
manila/{ceph.conf,cloud-user.keyring} cloud-user@172.25.250.151:
ceph.conf                                     100%  797   670.0KB/s   00:00
cloud-user.keyring                            100%   68    80.6KB/s   00:00
Install ceph-fuse on the production-server1 instance. Download the repository configuration file from http://materials.example.com/ceph.repo.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151
[cloud-user@production-server1 ~]$ sudo curl -s -f -o \
/etc/yum.repos.d/ceph.repo http://materials.example.com/ceph.repo
[cloud-user@production-server1 ~]$ sudo yum install ceph-fuse
...output omitted...
Total download size: 3.3 M
Installed size: 11 M
Is this ok [y/d/N]: y
...output omitted...
Complete!
Log off from production-server1.

[cloud-user@production-server1 ~]$ logout
[student@workstation ~(operator1-production)]$ 
Grant read and write permissions on production-share1 to client.cloud-user.

Grant read and write permissions on production-share1 to client.cloud-user.

[student@workstation ~(operator1-production)]$ manila access-allow \
--access-level rw \
production-share1 cephx cloud-user
+--------------+--------------------------------------+
| Property     | Value                                |
+--------------+--------------------------------------+
| access_key   | None                                 |
| share_id     | 84ce9714-2093-436b-ae70-4bd7fe48fa00 |
| created_at   | 2018-10-31T19:27:40.000000           |
| updated_at   | None                                 |
| access_type  | cephx                                |
| access_to    | cloud-user                           |
| access_level | rw                                   |
| state        | queued_to_apply                      |
| id           | ea602cdc-b303-4966-9a23-8fa07aad9969 |
+--------------+--------------------------------------+
Verify the access permissions on production-share1.

[student@workstation ~(operator1-production)]$ manila access-list \
--columns access_type,access_to,access_level,state \
production-share1
+-------------+------------+--------------+--------+
| Access_Type | Access_To  | Access_Level | State  |
+-------------+------------+--------------+--------+
| cephx       | cloud-user | rw           | active |
+-------------+------------+--------------+--------+
Access the production-share1 shared file system from the production-server1 instance. Authenticate as client.cloud-user using the key ring from /home/cloud-user/cloud-user.keyring while accessing production-share1. The appropriate Ceph cluster settings are in /home/cloud-user/ceph.conf. Create the file hello.txt with hello as the text message in the shared file system.

Open an SSH session to production-server1 and switch to root.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151
[cloud-user@production-server1 ~]$ sudo -i
[root@production-server1 ~]# 
Create /manila as the mount point for production-share1.

[root@production-server1 ~]# mkdir /manila
Open a new terminal on workstation as student and determine the export location of production-share1 as the operator1 tenant user.

[student@workstation ~]$ source operator1-production-rc
[student@workstation ~(operator1-production)]$ manila share-export-location-list \
production-share1 --columns Path
+------------------------------------------------------------------------+
| Path                                                                   |
+------------------------------------------------------------------------+
| 172.24.3.1:6789:/volumes/_nogroup/45306f38-73c9-44b3-a794-a84bdb0b25e3 |
+------------------------------------------------------------------------+
Take note of the export location because you are going to use it while mounting the production-share1 shared file system in the following step.

Return to the first terminal and mount production-share1 on /manila from production-server1. Authenticate as cloud-user.keyring using /home/cloud-user/cloud-user.keyring while mounting. Use /home/cloud-user/ceph.conf for the appropriate Ceph cluster settings. Use the same export location as in the preceding output.

[root@production-server1 ~]# ceph-fuse /manila \
--id=cloud-user --conf=/home/cloud-user/ceph.conf \
--keyring=/home/cloud-user/cloud-user.keyring \
--client-mountpoint=/volumes/_nogroup/45306f38-73c9-44b3-a794-a84bdb0b25e3
2018-10-31 16:30:39.637732 7f8163dc00c0 -1 init, newargv = 0x55855fcead80 newargc=9
ceph-fuse[1877]: starting ceph client
ceph-fuse[1877]: starting fuse
Verify that production-share1 is successfully mounted.

[root@production-server1 ~]# df -Th
Filesystem     Type            Size  Used Avail Use% Mounted on
/dev/vda1      xfs              10G  1.6G  8.5G  16% /
devtmpfs       devtmpfs        898M     0  898M   0% /dev
tmpfs          tmpfs           920M     0  920M   0% /dev/shm
tmpfs          tmpfs           920M   17M  903M   2% /run
tmpfs          tmpfs           920M     0  920M   0% /sys/fs/cgroup
tmpfs          tmpfs           184M     0  184M   0% /run/user/1001
ceph-fuse      fuse.ceph-fuse  1.0G     0  1.0G   0% /manila
Create /manila/hello.txt with hello as the text message in it.

[root@production-server1 ~]# echo hello > /manila/hello.txt
Unmount production-share1 from /manila.

[root@production-server1 ~]# umount /manila
Log off from production-server1.

[root@production-server1 ~]# logout
[cloud-user@production-server1 ~]$ logout
Connection to 172.25.250.151 closed.
[student@workstation ~(operator1-production)]$ 
Delete the production-server1 instance and create a new instance with the following parameters:

name: production-server2

flavor: default

key-name: operator1-keypair

nic: net-id=production-network1

nic: net-id=provider-storage

volume: production-volume1

user-data: /home/student/manila/user-data.file

NOTE
The user data file /home/student/manila/user-data.file contributes to configuring the second NIC of the instance.

Access the production-share1 shared file system from the new production-server2 instance to confirm that the text file you created in the shared file system in a previous step persists.

Delete the production-server1 instance.

[student@workstation ~(operator1-production)]$ openstack server delete \
production-server1
Create the new production-server2 instance with the aforementioned parameters.

[student@workstation ~(operator1-production)]$ openstack server create \
--flavor default \
--volume production-volume1 \
--key-name operator1-keypair \
--nic net-id=production-network1 \
--nic net-id=provider-storage \
--user-data /home/student/manila/user-data.file \
production-server2 --wait
...output omitted...
Associate the 172.25.250.151 floating IP address with production-server2.

[student@workstation ~(operator1-production)]$ openstack server add \
floating ip production-server2 172.25.250.151
Open an SSH session to production-server2 and switch to root.

[student@workstation ~(operator1-production)]$ ssh 172.25.250.151
[cloud-user@production-server2 ~]$ sudo -i
[root@production-server2 ~]# 
Mount production-share1 on /manila from production-server2. Authenticate as cloud-user.keyring using /home/cloud-user/cloud-user.keyring while mounting. Use /home/cloud-user/ceph.conf for the appropriate Ceph cluster settings. Use the same export location as in the preceding steps.

[root@production-server2 ~]# ceph-fuse /manila \
--id=cloud-user --conf=/home/cloud-user/ceph.conf \
--keyring=/home/cloud-user/cloud-user.keyring \
--client-mountpoint=/volumes/_nogroup/45306f38-73c9-44b3-a794-a84bdb0b25e3
ph-fuse[1377]: starting ceph client
2018-11-01 02:18:25.304052 7f59130e70c0 -1 init, newargv = 0x55c9235b6d80 newargc=9
ceph-fuse[1377]: starting fuse
Verify that production-share1 is successfully mounted.

[root@production-server2 ~]# df -Th
Filesystem     Type            Size  Used Avail Use% Mounted on
/dev/vda1      xfs              10G  1.6G  8.5G  16% /
devtmpfs       devtmpfs        898M     0  898M   0% /dev
tmpfs          tmpfs           920M     0  920M   0% /dev/shm
tmpfs          tmpfs           920M   17M  903M   2% /run
tmpfs          tmpfs           920M     0  920M   0% /sys/fs/cgroup
tmpfs          tmpfs           184M     0  184M   0% /run/user/1001
ceph-fuse      fuse.ceph-fuse  1.0G     0  1.0G   0% /manila
Confirm that the text file you created in the earlier steps in the production-share1 shared file system still persists with all of its content.

[root@production-server2 ~]# cat /manila/hello.txt
hello
Log off from production-server2 and delete the instance.

[root@production-server2 ~]# logout
[cloud-user@production-server2 ~]$ logout
Connection to 172.25.250.151 closed.
[student@workstation ~(operator1-production)]$ openstack server delete \
production-server2
Evaluation

On workstation, run the lab storage-review grade command to confirm success of this exercise.

[student@workstation ~]$ lab storage-review grade
Cleanup

On workstation, run the lab storage-review cleanup script to clean up this exercise.

[student@workstation ~]$ lab storage-review cleanup
This concludes the lab.